{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis from scratch\n",
        "Using modern NLP techniques on IMDB ratings data to classify text into positive and negative sentiment."
      ],
      "metadata": {
        "id": "o8DlizhKaXns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "-905J8j5arRW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "riXuewMpZuaj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import utils, optimizers, layers, models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data"
      ],
      "metadata": {
        "id": "LK-_KZF1avJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPnP8mP9awcW",
        "outputId": "26e5f023-919f-4836-e6af-41835094fe8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  32.5M      0  0:00:02  0:00:02 --:--:-- 32.5M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMMXmq2-azOC",
        "outputId": "084f8bb4-09ea-4c65-8c25-1addc35205d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb/test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQNxM6rIa4BB",
        "outputId": "6a6fb4b6-6d08-4ab7-f463-6361fdb2124c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "2v_Lku0Da8_0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating `tf.data.Dataset` instances from the directory structure\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "raw_train_ds = utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42069\n",
        ")\n",
        "\n",
        "raw_val_ds = utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42069\n",
        ")\n",
        "\n",
        "raw_test_ds = utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\",\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoaAHG1WbHZr",
        "outputId": "94cedbda-68aa-40bf-9e4c-7cb76e5f5cfe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Number of batches in raw_train_ds: 625\n",
            "Number of batches in raw_val_ds: 157\n",
            "Number of batches in raw_test_ds: 782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Previewing examples before tokenization\n",
        "for text, label in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(text.numpy()[i])\n",
        "    print(label.numpy()[i])\n",
        "    print(\"-------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD3r7R3bcPJ_",
        "outputId": "11d1a154-a2d1-4e1b-b485-f14f80aa926d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Well, maybe not immediately before the Rodney King riots, but even a few months before was timely enough. My parents said that they saw it and the next thing you know, the police got acquitted and LA got burned to the ground. It just goes to show the state of race relations in America. The plot has white Mack (Kevin Kline) and African-American Simon (Danny Glover) becoming friends after Simon saves Mack\\'s life in the black ghetto. Meanwhile, movie producer Davis (Steve Martin in a serious role) thinks that gratuitous violence is really cool...until he gets shot. There\\'s also some existentialism in the movie: Mack and his family come to realize that they aren\\'t living as they really want.<br /><br />It seems that \"Crash\" has somewhat renewed people\\'s interest in race relations, but this one came out much earlier. Maybe we\\'ll never be able to have stable race relations in this country. But either way, \"Grand Canyon\" is a great movie. It affirms Kevin Kline as my favorite actor. Also starring Mary McDonnell, Mary-Louise Parker and Alfre Woodard.'\n",
            "1\n",
            "-------\n",
            "b'My school\\'s drama club will be putting this show in the spring of 2002, and I can only hope we\\'re as good as this! I watched this film recently as sort of \"research\" for my role (Rosie Alvarez), and I\\'d just like to say, Vanessa Williams is the coolest!<br /><br />Wow! The casting for this movie was right-on (with one exception). Jason Alexander, oh my gawd, is there anything he can\\'t do? He was the most wonderful Albert Peterson ever - I especially loved all of his funny facial expressions and dancing during \"Put on a Happy Face!\" He is so great! Vanessa Williams, as I said before, is the coolest. She was a beautiful Rosie, and her transition from secretary to seductress was totally believable. Tyne Daly was hilarious as Albert\\'s obnoxious mother and George Wendt was superb as the annoyed Mr. McAfee (however I LOVED Paul Lynde\\'s performance in the 1963 version!). Brigitta Dau cracked me up as Ursula Merkle; she really hammed it up! And Marc Kudisch was an awesome Conrad Birdie...\"Suffer!\"<br /><br />There was only one casting that I didn\\'t understand, and, as you\\'ll see from previous comments, many other people didn\\'t understand. Chynna Phillips as Kim McAfee - what was that? I mean she\\'s really pretty and very talented, but...she looks a bit too old for the role. Eh, maybe I\\'m delusional.<br /><br />Okay well anyways, I highly recommend this movie. It\\'ll leave you smiling!<br /><br />'\n",
            "1\n",
            "-------\n",
            "b\"I always get frustrated by films that were obviously written by one gender. Especially when they obviously don't do enough research to find out when something not only doesn't ring true, but rings blatently false.<br /><br />The scene I am remembering is the one in the bathroom where Jack tells his football teammates that he got Diane pregnant. In no way, shape, or form would a guy ever cheer another guy getting a girl pregnant in high school. They might cheer about the guy having sex with the hot cheerleader, but I can also guarantee that the first the football team heard about it would not be at a urinal.<br /><br />It was obvious that this film didn't take itself so seriously, and it wasn't hideously bad, but come on!\"\n",
            "0\n",
            "-------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing data\n",
        "Cleaning and standardizing data"
      ],
      "metadata": {
        "id": "4YIkHM7QcjlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "iWV2h0WIcqKZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_text = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "  return tf.strings.regex_replace(\n",
        "      stripped_text, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "  )\n",
        "\n",
        "MAX_FEATURES = 20000\n",
        "EMBED_DIM = 128\n",
        "SEQ_LENGTH = 500\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=standardization,\n",
        "    max_tokens=MAX_FEATURES,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH\n",
        ")\n",
        "\n",
        "# Making a text-only dataset\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "eWJcT_z6csg4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text).cache().prefetch(buffer_size=1)\n",
        "val_ds = raw_val_ds.map(vectorize_text).cache().prefetch(buffer_size=1)\n",
        "test_ds = raw_test_ds.map(vectorize_text).cache().prefetch(buffer_size=1)"
      ],
      "metadata": {
        "id": "6i8DkeQ1dxpz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ],
      "metadata": {
        "id": "K3SmaiiteQnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(MAX_FEATURES, EMBED_DIM)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\")(x)\n",
        "model = models.Model(inputs, output, name=\"model_0_conv1d\")\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "LHEXFA-7eT4G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "EMOxxahcfPf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHTE-c3ufStz",
        "outputId": "4abdb0a6-b26f-46f5-a356-25172c47226f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 96s 138ms/step - loss: 0.5056 - accuracy: 0.7150 - val_loss: 0.3422 - val_accuracy: 0.8502\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2325 - accuracy: 0.9087 - val_loss: 0.2841 - val_accuracy: 0.8846\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1225 - accuracy: 0.9575 - val_loss: 0.3690 - val_accuracy: 0.8774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdf6041d150>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating on test data"
      ],
      "metadata": {
        "id": "3yypjAkqfd5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCUHLI89ff1B",
        "outputId": "de831430-7c4a-4315-ab3a-3f13ad31ccc7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 3s 4ms/step - loss: 0.4011 - accuracy: 0.8628\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4010775685310364, 0.8628399968147278]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving model to disk"
      ],
      "metadata": {
        "id": "tm6nr1v_gZu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump({'config': vectorize_layer.get_config(),\n",
        "             'weights': vectorize_layer.get_weights()},\n",
        "             open('model_0_conv1dvectorization.pkl', 'wb'))\n",
        "\n",
        "print(\"[INFO] Vectorizer saved\")\n",
        "\n",
        "model.save(\"model_0_conv1d.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2MQsmk-gceZ",
        "outputId": "401e4dbb-ebbe-4616-924b-2bcadfbb0a9c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Vectorizer saved\n"
          ]
        }
      ]
    }
  ]
}